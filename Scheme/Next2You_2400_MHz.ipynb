{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Next2You_2400_MHz.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_e-HJGgYphxc"
      },
      "source": [
        "# **Next2You functionality for 2.4 GHz**\n",
        "\n",
        "Codebase for the paper \"Next2You: Robust Copresence Detection Based on Channel State Information\" by Mikhail Fomichev, Luis F. Abanto-Leon, Max Stiegler, Alejandro Molina, Jakob Link, Matthias Hollick, in ACM Transactions on Internet of Things (2021)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YC1NiWGZpfNQ"
      },
      "source": [
        "# Import necessary modules for loading the CSI data \n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "import os\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nr-Pv23tcVg7"
      },
      "source": [
        "## Choose the CSI data from the specific experiment to be worked on. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MW-oMeLCsXhU"
      },
      "source": [
        "# Choose from which experiment the CSI data needs to be loaded, for the description of experiments see the paper \n",
        "# and/or the CSI data release: https://doi.org/10.5281/zenodo.5592335\n",
        "\n",
        "# experiments = [['Office/2400 MHz/18-02-19', 'Office/2400 MHz/19-02-19', 'Office/2400 MHz/19-02-19-night', 'Office/2400 MHz/20-02-19']]\n",
        "# experiments = [['Urban Flat/2400 MHz']]\n",
        "# experiments = [['Rural Flat/2400 MHz']]\n",
        "# experiments = [['Parking Cars/2400 MHz']]\n",
        "# experiments = [['Moving Cars/2400 MHz']]\n",
        "# experiments = [['6P/2400 MHz']]\n",
        "# experiments = [['Beacon/2400 MHz']]\n",
        "experiments = [['Power/2400 MHz']]"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "biC8vIUEv1Lu"
      },
      "source": [
        "## Normal training (e.g., Table 3 in the paper)\n",
        "\n",
        "We load the CSI data from the specified experiment in a stratified sampling fashion, deleting the CSI data of irrelevant null subcarriers (cf. Section 5.2 in the paper). \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jHh1JglzvEmp"
      },
      "source": [
        "# Delete CSI data of null subcarriers\n",
        "def delete_null(data):\n",
        "    if data.shape[1]==128:\n",
        "        null_carriers = np.array([0,32,33,34,35,29,30,31])\n",
        "        null_carriers = np.append(null_carriers, null_carriers+64)\n",
        "        data = np.delete(data, null_carriers, 1)\n",
        "    elif data.shape[1]==510:\n",
        "        null_carriers = np.array([0,1,123,124,125,126,127,128,129,130,131,132,133])\n",
        "        null_carriers = np.append(null_carriers, null_carriers+255)\n",
        "        data = np.delete(data, null_carriers, 1)\n",
        "    else:\n",
        "        null_carriers = np.array([0,1,123,124,125,126,127,128,129,130,131,132,133,255])\n",
        "        null_carriers = np.append(null_carriers, null_carriers+256)\n",
        "        data = np.delete(data, null_carriers, 1)\n",
        "    return data\n",
        "\n",
        "\n",
        "# Load CSI data in a stratified sampling fashion\n",
        "def check_and_write(data_arr, labels_arr, data0, labels0, data1, labels1, fold, previous):\n",
        "  if not data0 is None:\n",
        "    print('Adding to subsets (zeros): Collector ' + previous, str(data0.shape))\n",
        "    p0 = np.random.permutation(data0.shape[0])\n",
        "    data0 = data0[p0]\n",
        "    labels0 = labels0[p0]\n",
        "    step_size0 = data0.shape[0]//fold\n",
        "  if not data1 is None:\n",
        "    print('Adding to subsets (ones): Collector ' + previous, str(data1.shape))\n",
        "    p1 = np.random.permutation(data1.shape[0])\n",
        "    data1 = data1[p1]\n",
        "    labels1 = labels1[p1]\n",
        "    step_size1 = data1.shape[0]//fold\n",
        "  # watch out if step size is zero (should not happen for our experiments)\n",
        "  # p is used to shuffle one more time when adding stratums to subsets\n",
        "  p = np.random.permutation(fold)\n",
        "  if not not data_arr:\n",
        "    for i in range(fold-1):\n",
        "      print(\"Loading \" + 'data_arr ' + str(p[i]))\n",
        "      if not data0 is None:\n",
        "        print('Zero part: Adding to ' + 'data_arr ' + str(p[i]) + '; Former shape: ' + str(data_arr[p[i]].shape), 'Adding: ' + str(data0[i*step_size0:(i+1)*step_size0].shape))\n",
        "        data_arr[p[i]] = np.concatenate((data_arr[p[i]],data0[i*step_size0:(i+1)*step_size0]), axis=0)\n",
        "        labels_arr[p[i]] = np.concatenate((labels_arr[p[i]],labels0[i*step_size0:(i+1)*step_size0]), axis=0)\n",
        "      if not data1 is None:\n",
        "        print('One part: Adding to ' + 'data_arr ' + str(p[i]) + '; Former shape: ' + str(data_arr[p[i]].shape), 'Adding: ' + str(data1[i*step_size1:(i+1)*step_size1].shape))\n",
        "        data_arr[p[i]] = np.concatenate((data_arr[p[i]],data1[i*step_size1:(i+1)*step_size1]), axis=0)\n",
        "        labels_arr[p[i]] = np.concatenate((labels_arr[p[i]],labels1[i*step_size1:(i+1)*step_size1]), axis=0)\n",
        "    if not data0 is None:\n",
        "      print('Zero part: Adding to ' + 'csi_' + str(p[fold-1]) + '; Former shape: ' + str(data_arr[p[fold-1]].shape), 'Adding: ' + str(data0[(fold-1)*step_size0:].shape))\n",
        "      data_arr[p[fold-1]] = np.concatenate((data_arr[p[fold-1]],data0[(fold-1)*step_size0:]), axis=0)\n",
        "      labels_arr[p[fold-1]] = np.concatenate((labels_arr[p[fold-1]],labels0[(fold-1)*step_size0:]), axis=0)\n",
        "    if not data1 is None:\n",
        "      print('One part: Following added to ' + 'csi_' + str(p[fold-1]) + '; Former shape: ' + str(data_arr[p[fold-1]].shape), 'Adding: ' + str(data1[(fold-1)*step_size1:].shape))\n",
        "      data_arr[p[fold-1]] = np.concatenate((data_arr[p[fold-1]],data1[(fold-1)*step_size1:]), axis=0)\n",
        "      labels_arr[p[fold-1]] = np.concatenate((labels_arr[p[fold-1]],labels1[(fold-1)*step_size1:]), axis=0)\n",
        "  else:\n",
        "    if not data0 is None and not data1 is None:\n",
        "      for i in range(fold-1):\n",
        "        data_arr.append(data0[i*step_size0:(i+1)*step_size0])\n",
        "        labels_arr.append(labels0[i*step_size0:(i+1)*step_size0])\n",
        "        data_arr[i] = np.concatenate((data_arr[i],data1[i*step_size1:(i+1)*step_size1]), axis=0)\n",
        "        labels_arr[i] = np.concatenate((labels_arr[i],labels1[i*step_size1:(i+1)*step_size1]), axis=0)\n",
        "        print('Writing csi_' + str(p[i]), str(data_arr[i].shape))\n",
        "      data_arr.append(data0[(fold-1)*step_size0:])\n",
        "      labels_arr.append(labels0[(fold-1)*step_size0:])\n",
        "      data_arr[fold-1] = np.concatenate((data_arr[fold-1],data1[(fold-1)*step_size1:]), axis=0)\n",
        "      labels_arr[fold-1] = np.concatenate((labels_arr[fold-1],labels1[(fold-1)*step_size1:]), axis=0)\n",
        "      print('Writing csi_' + str(p[fold-1]), str(data_arr[fold-1].shape))\n",
        "    elif not data0 is None:\n",
        "      for i in range(fold-1):\n",
        "        data_arr.append(data0[i*step_size0:(i+1)*step_size0])\n",
        "        labels_arr.append(labels0[i*step_size0:(i+1)*step_size0])\n",
        "      data_arr.append(data0[(fold-1)*step_size0:])\n",
        "      labels_arr.append(labels0[(fold-1)*step_size0:])\n",
        "    elif not data1 is None:\n",
        "      for i in range(fold-1):\n",
        "        data_arr.append(data1[i*step_size1:(i+1)*step_size1])\n",
        "        labels_arr.append(labels1[i*step_size1:(i+1)*step_size1])\n",
        "      data_arr.append(data1[(fold-1)*step_size1:])\n",
        "      labels_arr.append(labels1[(fold-1)*step_size1:])\n",
        "\n",
        "\n",
        "# Form training and test sets of CSI data, important:\n",
        "# (1) On Google Drive the CSI data is supposed to be stored at: My Drive/csi/data/ (e.g., My Drive/csi/data/6P)\n",
        "# (2) (Un-)comment lines 107–109 to use only CSI magnitudes or phases of the CSI data, by default both are used\n",
        "def validation_sets_from_data(directories, fold, seed):\n",
        "  np.random.seed(seed)\n",
        "  for directory in directories:\n",
        "    path = '/content/drive/My Drive/csi-zia/data/' + directory\n",
        "    relpath = '/content/drive/My Drive/csi-zia/data/' + directory + '/'\n",
        "    previous = 'None'\n",
        "    data0 = None\n",
        "    data1 = None\n",
        "    labels0 = None\n",
        "    labels1 = None\n",
        "    for filename in sorted(os.listdir(path)):\n",
        "      if filename.endswith('.txt') and 'csi' in filename:\n",
        "        #check if file belongs to new collector:\n",
        "        collector = filename.split('_')[0]\n",
        "        if collector == '':\n",
        "          collector = filename.split('_')[1]\n",
        "        if not previous==collector:\n",
        "          if not previous=='None':\n",
        "            check_and_write(csi_data, csi_labels, data0, labels0, data1, labels1, fold, previous)\n",
        "          data0 = None\n",
        "          data1 = None\n",
        "          previous=collector\n",
        "        #handle new file:  \n",
        "        new = delete_null(np.loadtxt(relpath + filename))\n",
        "        new_labels = np.loadtxt(relpath + filename.replace('csi','labels')) \n",
        "        # new_flatten = new[:,:new.shape[1]//2]                                     # use only CSI magnitudes\n",
        "        # new_flatten = new[:,new.shape[1]//2:]                                     # use only CSI phases\n",
        "        new_flatten = new                                                           # use both CSI magnitudes and phases                                     \n",
        "        if new_labels[0]==1: \n",
        "          if data1 is None:\n",
        "            data1 = new_flatten\n",
        "            labels1 = new_labels\n",
        "          else:  \n",
        "            data1 = np.concatenate((data1,new_flatten), axis=0)\n",
        "            labels1 = np.concatenate((labels1,new_labels), axis=0)\n",
        "        else:\n",
        "          if data0 is None:\n",
        "            data0 = new_flatten\n",
        "            labels0 = new_labels\n",
        "          else:  \n",
        "            data0 = np.concatenate((data0,new_flatten), axis=0)\n",
        "            labels0 = np.concatenate((labels0,new_labels), axis=0)\n",
        "    check_and_write(csi_data, csi_labels, data0, labels0, data1, labels1, fold, previous)\n",
        "  print('Data successfully rearranged.')\n",
        "\n",
        "\n",
        "# *****************************************************************************#\n",
        "\n",
        "# Forming the data by calling the above functions\n",
        "csi_data = []       # CSI data\n",
        "csi_labels = []     # CSI labels: 1 - for colocated devices, 0 - for non-colocated devices\n",
        "\n",
        "# Iterate over the experiments (see above Office data is composed from multiple data sets), important:\n",
        "# (1) We set the number of folds to 5, i.e., for 5-fold CV in 'validation_sets_from_data' and \n",
        "# (2) The seed to shuffle the data to 123, for reproducibility (same function)\n",
        "for experiment in experiments:\n",
        "  print(experiment)\n",
        "  validation_sets_from_data(experiment, 5, 123)\n",
        "  print(experiment[0].split('/')[0] + '/' + experiment[0].split('/')[1] + ' subsets finished') \n",
        "\n",
        "print(csi_data[0].shape, csi_data[1].shape, csi_data[2].shape, csi_data[3].shape, csi_data[4].shape)\n",
        "print(csi_labels[0].shape, csi_labels[1].shape, csi_labels[2].shape, csi_labels[3].shape, csi_labels[4].shape)\n",
        "print('')\n",
        "print('')\n",
        "\n",
        "# Forming the training and test datasets\n",
        "for i in range(1):\n",
        "  test_data = csi_data[i]\n",
        "  test_labels = csi_labels[i]\n",
        "\n",
        "  train_data = csi_data[(i+1)%5]\n",
        "  train_labels = csi_labels[(i+1)%5]\n",
        "  for k in range(5):\n",
        "    if not k==i and not k==(i+1)%5:\n",
        "      train_data = np.concatenate((train_data, csi_data[k]), axis=0)\n",
        "      train_labels = np.concatenate((train_labels, csi_labels[k]), axis=0)\n",
        "  print(train_data.shape, train_labels.shape)\n",
        "  print(test_data.shape, test_labels.shape)\n",
        "\n",
        "  # Normalizing CSI data using variance scaling\n",
        "  mean = np.mean(train_data, axis=0)\n",
        "  train_data = train_data - mean\n",
        "  std = np.std(train_data, axis=0)\n",
        "  train_data = train_data / std\n",
        "  test_data = (test_data - mean) / std\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UQJCVWh6FxVX"
      },
      "source": [
        "**Bottom line in the above cell:** The 'train_data' and 'test_data' contain the stratified data from the folds in the ratio 80% to 20%, respectively. \n",
        "\n",
        "We use this setup for subsequent training to speed up the computation. For a 5-fold cross validation (CV) described in the paper, apply the below code; when dealing with individual models, we reported the results corresponding to the best model obtainable from the 5-fold CV.\n",
        "\n",
        "The results when using the 80% to 20% ratio vs. the full 5-fold CV differ marginally (i.e., the former shows slightly lower Area Under the ROC Curve – AUC)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qkzyfzqgJhRD"
      },
      "source": [
        "## Train a neural network model with Keras (cf. Figure 7 in the paper for the structure)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xpnJ7LbBJXvD"
      },
      "source": [
        "# Import necessary modules\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "\n",
        "# Convert soft predictions to binary\n",
        "def make_bin_predictions(soft_pred):\n",
        "  # Create binary predicitions array\n",
        "  bin_pred = np.zeros((soft_pred.shape[0]), dtype=np.int8)\n",
        "\n",
        "  # Convert from soft predictions to binary predictions\n",
        "  for i in range(soft_pred.shape[0]):\n",
        "    if soft_pred[i,1] > soft_pred[i,0]:\n",
        "      bin_pred[i] = 1\n",
        "\n",
        "  return bin_pred\n",
        "\n",
        "\n",
        "# Train a neural network model and make predictions on the data\n",
        "def nn_model_train_predict(data, labels, test, test_labels, num_epochs=35):\n",
        "  # Define a model\n",
        "  model = keras.Sequential()\n",
        "  model.add(keras.layers.Dense(500, input_shape=(data.shape[1],), activation=tf.nn.leaky_relu))\n",
        "  model.add(keras.layers.Dropout(0.2))\n",
        "  model.add(keras.layers.Dense(300, activation=tf.nn.leaky_relu)) \n",
        "  model.add(keras.layers.Dropout(0.2))\n",
        "  model.add(keras.layers.Dense(100, activation=tf.nn.leaky_relu)) \n",
        "  model.add(keras.layers.Dropout(0.2))\n",
        "  model.add(keras.layers.Dense(20, activation=tf.nn.leaky_relu)) \n",
        "  model.add(keras.layers.Dropout(0.2))\n",
        "  model.add(keras.layers.Dense(2, activation=tf.nn.softmax))\n",
        "  model.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "  \n",
        "  # Perform training\n",
        "  model.fit(data, labels, verbose=2, epochs=num_epochs, batch_size=32)\n",
        "\n",
        "  # Get training and test accuracies\n",
        "  train_loss, train_acc = model.evaluate(data, labels)\n",
        "  test_loss, test_acc = model.evaluate(test, test_labels)\n",
        "\n",
        "  # Make training and test soft predictions\n",
        "  train_pred_soft = model.predict(data)\n",
        "  test_pred_soft = model.predict(test)\n",
        "\n",
        "  # Convert soft predictions to binary predictions\n",
        "  train_pred = make_bin_predictions(train_pred_soft)\n",
        "  test_pred = make_bin_predictions(test_pred_soft)\n",
        "\n",
        "  return train_acc, test_acc, train_pred, test_pred, model"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fEIByILuMV9s"
      },
      "source": [
        "Train on the CSI data (i.e., 80% to 20% setup) and make copresence predictions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2qofzqJgME0S"
      },
      "source": [
        "train_acc, test_acc, train_pred, test_pred, model = nn_model_train_predict(train_data, train_labels, test_data, test_labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LzkDm43sMkrB"
      },
      "source": [
        "Display the results: accuracy and AUC."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sX6hPi1iMl0a"
      },
      "source": [
        "print('Accuracy: %.4f' % test_acc)\n",
        "print('AUC: %.4f' % roc_auc_score(test_labels, test_pred))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sAce0ZSXS-Lb"
      },
      "source": [
        "Optionally you can save the resulting neural network model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jawj370pRpBq"
      },
      "source": [
        "# Where to save the model and its name\n",
        "model_path = '/content/drive/My Drive/csi-zia/results/'\n",
        "model_name = 'my-model'\n",
        "\n",
        "# Save Keras model\n",
        "model.save(model_path + model_name + '.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kgcNkODMTVHo"
      },
      "source": [
        "And (again optionally) load it back on."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tiqqxSOpTXiC"
      },
      "source": [
        "# Function to load the model\n",
        "from keras.models import load_model\n",
        "\n",
        "# From where to load the model \n",
        "model_path = '/content/drive/My Drive/csi-zia/results/'\n",
        "model_name = 'my-model'\n",
        "\n",
        "# Loaded model\n",
        "l_model = tf.keras.models.load_model(model_path + model_name + '.h5', \n",
        "                                   custom_objects={'leaky_relu': tf.nn.leaky_relu})\n",
        "\n",
        "# Verify that the model is correct returing the same AUC\n",
        "print('AUC = %.4f' % (roc_auc_score(test_labels, \n",
        "                                    make_bin_predictions(model.predict(test_data)))))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W9_C2UgiWzSg"
      },
      "source": [
        "## Training using a 5-fold CV.\n",
        "\n",
        "**Prerequisite:** Cell 'Normal training' must be execute beforehand. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OMkUsb3_W-lN"
      },
      "source": [
        "# Lists storing accuracies and AUCs of cross-validated folds\n",
        "cv_acc = []\n",
        "cv_auc = []\n",
        "\n",
        "# Loop: we have five folds\n",
        "for i in range(0, 5):\n",
        "  # Set the test data and labels: one fold\n",
        "  test_data = csi_data[i]\n",
        "  test_labels = csi_labels[i] \n",
        "\n",
        "  # Initialize train data and labels\n",
        "  train_data = None\n",
        "  train_labels = None\n",
        "\n",
        "  # Set train data and lebels: four folds\n",
        "  for j in range(0, 5):\n",
        "    if j != i:\n",
        "      if train_data is None:\n",
        "        train_data = csi_data[j]\n",
        "        train_labels = csi_labels[j]\n",
        "      else:\n",
        "        train_data = np.concatenate((train_data, csi_data[j]), axis=0)\n",
        "        train_labels = np.concatenate((train_labels, csi_labels[j]), axis=0)\n",
        "\n",
        "  print(train_data.shape, train_labels.shape)\n",
        "  print(test_data.shape, test_labels.shape)\n",
        "\n",
        "  # Normalize CSI data prior to training with variance scaling\n",
        "  mean = np.mean(train_data, axis=0)\n",
        "  train_data = train_data - mean\n",
        "  std = np.std(train_data, axis=0)\n",
        "  train_data = train_data / std\n",
        "  test_data = (test_data - mean) / std\n",
        "\n",
        "  # Set the number of epochs the neural network will be trained for\n",
        "  n_epochs = 25\n",
        "\n",
        "  # Do the training, prediction\n",
        "  _, test_acc, _, test_pred, model = nn_model_train_predict(train_data, train_labels, test_data, test_labels, n_epochs)\n",
        "\n",
        "  # Display accuracy and AUC\n",
        "  print('Accuracy: %.4f' % test_acc)\n",
        "  test_auc = roc_auc_score(test_labels, test_pred)\n",
        "  print('AUC: %.4f' % test_auc)\n",
        "\n",
        "  # Save accuracy and AUC\n",
        "  cv_acc.append(test_acc)\n",
        "  cv_auc.append(test_auc)\n",
        "  \n",
        "  # Optionally save 'model' as shown above for future use or computing EERs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dBR_JQfC64Qz"
      },
      "source": [
        "Display the results: accuracy and AUC."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UBn4ZzY82-xX"
      },
      "source": [
        "print('Accuracy: %.4f' % np.mean(np.array(cv_acc)))\n",
        "print('AUC: %.4f' % np.mean(np.array(cv_auc)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O_EKixyiEW_g"
      },
      "source": [
        "## Prepare data for different time of day (tod) model training  in the Office scenario (cf. Section 6.2 in the paper).\n",
        "\n",
        "**Note:** functions 'delete_null' and 'check_and_write' are the same as in the above 'Normal training' cell. However, I have decided to keep the cells self-contained. \n",
        "\n",
        "The resulting data split is in the form of 80% to 20% ratio, as described in the 'Normal training' cell. To run with the 5-fold CV, adapt the code from the 'Training using a 5-fold CV' cell. However, the results should be marginally different. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QlcnpbOpE2lM"
      },
      "source": [
        "# Office data\n",
        "experiments = ['Office/2400 MHz/18-02-19', 'Office/2400 MHz/19-02-19', 'Office/2400 MHz/19-02-19-night', 'Office/2400 MHz/20-02-19']\n",
        "\n",
        "# Delete CSI data of null subcarriers\n",
        "def delete_null(data):\n",
        "    if data.shape[1]==128:\n",
        "        null_carriers = np.array([0,32,33,34,35,29,30,31])\n",
        "        null_carriers = np.append(null_carriers, null_carriers+64)\n",
        "        data = np.delete(data, null_carriers, 1)\n",
        "    elif data.shape[1]==510:\n",
        "        null_carriers = np.array([0,1,123,124,125,126,127,128,129,130,131,132,133])\n",
        "        null_carriers = np.append(null_carriers, null_carriers+255)\n",
        "        data = np.delete(data, null_carriers, 1)\n",
        "    else:\n",
        "        null_carriers = np.array([0,1,123,124,125,126,127,128,129,130,131,132,133,255])\n",
        "        null_carriers = np.append(null_carriers, null_carriers+256)\n",
        "        data = np.delete(data, null_carriers, 1)\n",
        "    return data\n",
        "\n",
        "\n",
        "# Load CSI data in a stratified sampling fashion\n",
        "def check_and_write(data_arr, labels_arr, data0, labels0, data1, labels1, fold, previous):\n",
        "  if not data0 is None:\n",
        "    print('Adding to subsets (zeros): Collector ' + previous, str(data0.shape))\n",
        "    p0 = np.random.permutation(data0.shape[0])\n",
        "    data0 = data0[p0]\n",
        "    labels0 = labels0[p0]\n",
        "    step_size0 = data0.shape[0]//fold\n",
        "  if not data1 is None:\n",
        "    print('Adding to subsets (ones): Collector ' + previous, str(data1.shape))\n",
        "    p1 = np.random.permutation(data1.shape[0])\n",
        "    data1 = data1[p1]\n",
        "    labels1 = labels1[p1]\n",
        "    step_size1 = data1.shape[0]//fold\n",
        "  # watch out if step size is zero (should not happen for our experiments)\n",
        "  # p is used to shuffle one more time when adding stratums to subsets\n",
        "  p = np.random.permutation(fold)\n",
        "  if not not data_arr:\n",
        "    for i in range(fold-1):\n",
        "      print(\"Loading \" + 'data_arr ' + str(p[i]))\n",
        "      if not data0 is None:\n",
        "        print('Zero part: Adding to ' + 'data_arr ' + str(p[i]) + '; Former shape: ' + str(data_arr[p[i]].shape), 'Adding: ' + str(data0[i*step_size0:(i+1)*step_size0].shape))\n",
        "        data_arr[p[i]] = np.concatenate((data_arr[p[i]],data0[i*step_size0:(i+1)*step_size0]), axis=0)\n",
        "        labels_arr[p[i]] = np.concatenate((labels_arr[p[i]],labels0[i*step_size0:(i+1)*step_size0]), axis=0)\n",
        "      if not data1 is None:\n",
        "        print('One part: Adding to ' + 'data_arr ' + str(p[i]) + '; Former shape: ' + str(data_arr[p[i]].shape), 'Adding: ' + str(data1[i*step_size1:(i+1)*step_size1].shape))\n",
        "        data_arr[p[i]] = np.concatenate((data_arr[p[i]],data1[i*step_size1:(i+1)*step_size1]), axis=0)\n",
        "        labels_arr[p[i]] = np.concatenate((labels_arr[p[i]],labels1[i*step_size1:(i+1)*step_size1]), axis=0)\n",
        "    if not data0 is None:\n",
        "      print('Zero part: Adding to ' + 'csi_' + str(p[fold-1]) + '; Former shape: ' + str(data_arr[p[fold-1]].shape), 'Adding: ' + str(data0[(fold-1)*step_size0:].shape))\n",
        "      data_arr[p[fold-1]] = np.concatenate((data_arr[p[fold-1]],data0[(fold-1)*step_size0:]), axis=0)\n",
        "      labels_arr[p[fold-1]] = np.concatenate((labels_arr[p[fold-1]],labels0[(fold-1)*step_size0:]), axis=0)\n",
        "    if not data1 is None:\n",
        "      print('One part: Following added to ' + 'csi_' + str(p[fold-1]) + '; Former shape: ' + str(data_arr[p[fold-1]].shape), 'Adding: ' + str(data1[(fold-1)*step_size1:].shape))\n",
        "      data_arr[p[fold-1]] = np.concatenate((data_arr[p[fold-1]],data1[(fold-1)*step_size1:]), axis=0)\n",
        "      labels_arr[p[fold-1]] = np.concatenate((labels_arr[p[fold-1]],labels1[(fold-1)*step_size1:]), axis=0)\n",
        "  else:\n",
        "    if not data0 is None and not data1 is None:\n",
        "      for i in range(fold-1):\n",
        "        data_arr.append(data0[i*step_size0:(i+1)*step_size0])\n",
        "        labels_arr.append(labels0[i*step_size0:(i+1)*step_size0])\n",
        "        data_arr[i] = np.concatenate((data_arr[i],data1[i*step_size1:(i+1)*step_size1]), axis=0)\n",
        "        labels_arr[i] = np.concatenate((labels_arr[i],labels1[i*step_size1:(i+1)*step_size1]), axis=0)\n",
        "        print('Writing csi_' + str(p[i]), str(data_arr[i].shape))\n",
        "      data_arr.append(data0[(fold-1)*step_size0:])\n",
        "      labels_arr.append(labels0[(fold-1)*step_size0:])\n",
        "      data_arr[fold-1] = np.concatenate((data_arr[fold-1],data1[(fold-1)*step_size1:]), axis=0)\n",
        "      labels_arr[fold-1] = np.concatenate((labels_arr[fold-1],labels1[(fold-1)*step_size1:]), axis=0)\n",
        "      print('Writing csi_' + str(p[fold-1]), str(data_arr[fold-1].shape))\n",
        "    elif not data0 is None:\n",
        "      for i in range(fold-1):\n",
        "        data_arr.append(data0[i*step_size0:(i+1)*step_size0])\n",
        "        labels_arr.append(labels0[i*step_size0:(i+1)*step_size0])\n",
        "      data_arr.append(data0[(fold-1)*step_size0:])\n",
        "      labels_arr.append(labels0[(fold-1)*step_size0:])\n",
        "    elif not data1 is None:\n",
        "      for i in range(fold-1):\n",
        "        data_arr.append(data1[i*step_size1:(i+1)*step_size1])\n",
        "        labels_arr.append(labels1[i*step_size1:(i+1)*step_size1])\n",
        "      data_arr.append(data1[(fold-1)*step_size1:])\n",
        "      labels_arr.append(labels1[(fold-1)*step_size1:])\n",
        "\n",
        "\n",
        "# Form training and test sets of CSI data, important:\n",
        "# (1) On Google Drive the CSI data is supposed to be stored at: My Drive/csi/data/ (e.g., My Drive/csi/data/Office/2400 MHz/18-02-19)\n",
        "# (2) Load CSI data in a stratified sampling fashion per time of day, i.e., m - morning, a - afternoon, e - evening, n - night\n",
        "def validation_sets_from_data(directory, fold, seed):\n",
        "  path = '/content/drive/My Drive/csi-zia/data/' + directory\n",
        "  relpath = '/content/drive/My Drive/csi-zia/data/' + directory + '/'\n",
        "  previous = 'None'\n",
        "  data0 = None\n",
        "  data1 = None\n",
        "  labels0 = None\n",
        "  labels1 = None\n",
        "  np.random.seed(seed)\n",
        "  for filename in sorted(os.listdir(path)):\n",
        "    if filename.endswith('.txt') and 'csi' in filename:\n",
        "      #check if file belongs to new collector:\n",
        "      collector = filename.split('_')[0]\n",
        "      if collector == '':\n",
        "        collector = filename.split('_')[1]\n",
        "      if not previous==collector:\n",
        "        if not previous=='None':\n",
        "          if '-m-' in previous:\n",
        "            check_and_write(m_data, m_labels, data0, labels0, data1, labels1, fold, previous)\n",
        "          elif '-a-' in previous:\n",
        "            check_and_write(a_data, a_labels, data0, labels0, data1, labels1, fold, previous)\n",
        "          elif '-e-' in previous:\n",
        "            check_and_write(e_data, e_labels, data0, labels0, data1, labels1, fold, previous)\n",
        "          else:\n",
        "            check_and_write(n_data, n_labels, data0, labels0, data1, labels1, fold, previous)\n",
        "        data0 = None\n",
        "        data1 = None\n",
        "        previous=collector\n",
        "      #handle new file:  \n",
        "      new_labels = np.loadtxt(relpath + filename.replace('csi','labels'))\n",
        "      new = np.loadtxt(relpath + filename)\n",
        "      if new_labels[0]==1: \n",
        "        if data1 is None:\n",
        "          data1 = np.loadtxt(relpath + filename)\n",
        "          labels1 = np.loadtxt(relpath + filename.replace('csi','labels'))\n",
        "        else:  \n",
        "          data1 = np.concatenate((data1,new), axis=0)\n",
        "          labels1 = np.concatenate((labels1,new_labels), axis=0)\n",
        "      else:\n",
        "        if data0 is None:\n",
        "          data0 = np.loadtxt(relpath + filename)\n",
        "          labels0 = np.loadtxt(relpath + filename.replace('csi','labels'))\n",
        "        else:  \n",
        "          data0 = np.concatenate((data0,new), axis=0)\n",
        "          labels0 = np.concatenate((labels0,new_labels), axis=0)\n",
        "  if '-m-' in previous:\n",
        "    check_and_write(m_data, m_labels, data0, labels0, data1, labels1, fold, previous)\n",
        "  elif '-a-' in previous:\n",
        "    check_and_write(a_data, a_labels, data0, labels0, data1, labels1, fold, previous)\n",
        "  elif '-e-' in previous:\n",
        "    check_and_write(e_data, e_labels, data0, labels0, data1, labels1, fold, previous)\n",
        "  else:\n",
        "    check_and_write(n_data, n_labels, data0, labels0, data1, labels1, fold, previous)\n",
        "  print('Data successfully rearranged.')\n",
        "\n",
        "\n",
        "# *****************************************************************************#\n",
        "\n",
        "# Forming the data by calling the above functions\n",
        "m_data = [] # CSI data per time of day: morning, afternoon, evening, night\n",
        "a_data = []\n",
        "e_data = []\n",
        "n_data = []\n",
        "\n",
        "m_labels = [] # CSI labels: 1 - for colocated devices, 0 - for non-colocated devices\n",
        "a_labels = []\n",
        "e_labels = []\n",
        "n_labels = []\n",
        "\n",
        "# Iterate over the experiments (see above Office data is composed from multiple data sets), important:\n",
        "# (1) We set the number of folds to 5, i.e., for 5-fold CV in 'validation_sets_from_data' and \n",
        "# (2) The seed to shuffle the data to 123, for reproducibility (same function)\n",
        "for experiment in experiments:\n",
        "  print(experiment)\n",
        "  validation_sets_from_data(experiment, 5, 123)\n",
        "  print(experiment + ' subsets finished')          \n",
        "\n",
        "print(m_data[0].shape, m_data[1].shape, m_data[2].shape, m_data[3].shape, m_data[4].shape)\n",
        "print(m_labels[0].shape, m_labels[1].shape, m_labels[2].shape, m_labels[3].shape, m_labels[4].shape)\n",
        "print(a_data[0].shape, a_data[1].shape, a_data[2].shape, a_data[3].shape, a_data[4].shape)\n",
        "print(a_labels[0].shape, a_labels[1].shape, a_labels[2].shape, a_labels[3].shape, a_labels[4].shape)\n",
        "print(e_data[0].shape, e_data[1].shape, e_data[2].shape, e_data[3].shape, e_data[4].shape)\n",
        "print(e_labels[0].shape, e_labels[1].shape, e_labels[2].shape, e_labels[3].shape, e_labels[4].shape)\n",
        "print(n_data[0].shape, n_data[1].shape, n_data[2].shape, n_data[3].shape, n_data[4].shape)\n",
        "print(n_labels[0].shape, n_labels[1].shape, n_labels[2].shape, n_labels[3].shape, n_labels[4].shape)\n",
        "print('')\n",
        "print('')\n",
        "\n",
        "# Forming the training and test datasets for each time of day CSI data\n",
        "for i in range(1):\n",
        "  m_test_data = delete_null(m_data[i])\n",
        "  a_test_data = delete_null(a_data[i])\n",
        "  e_test_data = delete_null(e_data[i])\n",
        "  n_test_data = delete_null(n_data[i])\n",
        "\n",
        "  m_test_labels = m_labels[i]\n",
        "  a_test_labels = a_labels[i]\n",
        "  e_test_labels = e_labels[i]\n",
        "  n_test_labels = n_labels[i]\n",
        "\n",
        "  m_train_data = delete_null(m_data[(i+1)%5])\n",
        "  a_train_data = delete_null(a_data[(i+1)%5])\n",
        "  e_train_data = delete_null(e_data[(i+1)%5])\n",
        "  n_train_data = delete_null(n_data[(i+1)%5])\n",
        "\n",
        "  m_train_labels = m_labels[(i+1)%5]\n",
        "  a_train_labels = a_labels[(i+1)%5]\n",
        "  e_train_labels = e_labels[(i+1)%5]\n",
        "  n_train_labels = n_labels[(i+1)%5]\n",
        "\n",
        "  for k in range(5):\n",
        "    if not k==i and not k==(i+1)%5:\n",
        "      m_train_data = np.concatenate((m_train_data,delete_null(m_data[k])), axis=0)\n",
        "      m_train_labels = np.concatenate((m_train_labels, m_labels[k]), axis=0)\n",
        "      a_train_data = np.concatenate((a_train_data,delete_null(a_data[k])), axis=0)\n",
        "      a_train_labels = np.concatenate((a_train_labels, a_labels[k]), axis=0)\n",
        "      e_train_data = np.concatenate((e_train_data,delete_null(e_data[k])), axis=0)\n",
        "      e_train_labels = np.concatenate((e_train_labels, e_labels[k]), axis=0)\n",
        "      n_train_data = np.concatenate((n_train_data,delete_null(n_data[k])), axis=0)\n",
        "      n_train_labels = np.concatenate((n_train_labels, n_labels[k]), axis=0)\n",
        "  \n",
        "  print(m_train_data.shape, m_train_labels.shape, a_train_data.shape, a_train_labels.shape, e_train_data.shape, e_train_labels.shape, n_train_data.shape, n_train_labels.shape)\n",
        "  print(m_test_data.shape, m_test_labels.shape, a_test_data.shape, a_test_labels.shape, e_test_data.shape, e_test_labels.shape, n_test_data.shape, n_test_labels.shape)\n",
        "\n",
        "  train_data = []\n",
        "  test_data = []\n",
        "  train_labels = []\n",
        "  test_labels = []\n",
        "  \n",
        "  # Perform variance scaling normalization of CSI data\n",
        "  mean = np.mean(m_train_data, axis=0)\n",
        "  m_train_data = m_train_data - mean\n",
        "  std = np.std(m_train_data, axis=0)\n",
        "  m_train_data = m_train_data / std\n",
        "  m_test_data = (m_test_data - mean) / std\n",
        "  # train_data.append(m_train_data[:,:m_train_data.shape[1]//2])                  # Exclude CSI phase (just for illustrative purposes, e.g., for future use)\n",
        "  # test_data.append(m_test_data[:,:m_train_data.shape[1]//2])\n",
        "  train_data.append(m_train_data)\n",
        "  test_data.append(m_test_data)               \n",
        "  train_labels.append(m_train_labels)\n",
        "  test_labels.append(m_test_labels)\n",
        "  \n",
        "  mean = np.mean(a_train_data, axis=0)\n",
        "  a_train_data = a_train_data - mean\n",
        "  std = np.std(a_train_data, axis=0)\n",
        "  a_train_data = a_train_data / std\n",
        "  a_test_data = (a_test_data - mean) / std\n",
        "  # train_data.append(a_train_data[:,:m_train_data.shape[1]//2])                  # Exclude CSI phase (just for illustrative purposes, e.g., for future use)\n",
        "  # test_data.append(a_test_data[:,:m_train_data.shape[1]//2])\n",
        "  train_data.append(a_train_data)\n",
        "  test_data.append(a_test_data)\n",
        "  train_labels.append(a_train_labels)\n",
        "  test_labels.append(a_test_labels)\n",
        "  \n",
        "  mean = np.mean(e_train_data, axis=0)\n",
        "  e_train_data = e_train_data - mean\n",
        "  std = np.std(e_train_data, axis=0)\n",
        "  e_train_data = e_train_data / std\n",
        "  e_test_data = (e_test_data - mean) / std\n",
        "  # train_data.append(e_train_data[:,:m_train_data.shape[1]//2])                  # Exclude CSI phase (just for illustrative purposes, e.g., for future use)\n",
        "  # test_data.append(e_test_data[:,:m_train_data.shape[1]//2])\n",
        "  train_data.append(e_train_data)                       \n",
        "  test_data.append(e_test_data)\n",
        "  train_labels.append(e_train_labels)\n",
        "  test_labels.append(e_test_labels)\n",
        "\n",
        "  mean = np.mean(n_train_data, axis=0)\n",
        "  n_train_data = n_train_data - mean\n",
        "  std = np.std(n_train_data, axis=0)\n",
        "  n_train_data = n_train_data / std\n",
        "  n_test_data = (n_test_data - mean) / std\n",
        "  # train_data.append(n_train_data[:,:m_train_data.shape[1]//2])                  # Exclude CSI phase (just for illustrative purposes, e.g., for future use)\n",
        "  # test_data.append(n_test_data[:,:m_train_data.shape[1]//2])\n",
        "  train_data.append(n_train_data)\n",
        "  test_data.append(n_test_data)\n",
        "  train_labels.append(n_train_labels)\n",
        "  test_labels.append(n_test_labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hd1KUNbzOJwB"
      },
      "source": [
        "Do the training on the time of day (tod) CSI data. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mptpHAEvOK1k"
      },
      "source": [
        "# List to store accuracies and predictions\n",
        "tod_test_pred = []\n",
        "tod_test_acc = []\n",
        "tod_models = []\n",
        "\n",
        "# Train models for each time of day\n",
        "for i in range(len(train_data)):\n",
        "  _, test_acc, _, test_pred, model = nn_model_train_predict(train_data[i], train_labels[i], test_data[i], test_labels[i], 25)\n",
        "  tod_test_acc.append(test_acc)\n",
        "  tod_test_pred.append(test_pred)\n",
        "  tod_models.append(model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-b0ALoHJPB1h"
      },
      "source": [
        "Display the results: accuracy and AUC.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-wrfGcbUPCZp"
      },
      "source": [
        "# Print accuracy and AUC for each time of day model\n",
        "for acc, pred, labels in zip(tod_test_acc, tod_test_pred, test_labels):\n",
        "  print('Accuracy: %.4f' % acc)\n",
        "  print('AUC: %.4f' % roc_auc_score(labels, pred))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jc4aQEam7ret"
      },
      "source": [
        "## Phase sanitization (cf. Section 6.4 in the paper).\n",
        "\n",
        "**Note:** functions 'delete_null' and 'check_and_write' are the same as in the above 'Normal training' cell. However, I have decided to keep the cells self-contained. \n",
        "\n",
        "The resulting data split is in the form of 80% to 20% ratio, as described in the 'Normal training' cell.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QWFp10Ql8L0b"
      },
      "source": [
        "# Delete CSI data of null subcarriers\n",
        "def delete_null(data):\n",
        "    if data.shape[1]==128:\n",
        "        null_carriers = np.array([0,32,33,34,35,29,30,31])\n",
        "        null_carriers = np.append(null_carriers, null_carriers+64)\n",
        "        data = np.delete(data, null_carriers, 1)\n",
        "    elif data.shape[1]==510:\n",
        "        null_carriers = np.array([0,1,123,124,125,126,127,128,129,130,131,132,133])\n",
        "        null_carriers = np.append(null_carriers, null_carriers+255)\n",
        "        data = np.delete(data, null_carriers, 1)\n",
        "    else:\n",
        "        null_carriers = np.array([0,1,123,124,125,126,127,128,129,130,131,132,133,255])\n",
        "        null_carriers = np.append(null_carriers, null_carriers+256)\n",
        "        data = np.delete(data, null_carriers, 1)\n",
        "    return data\n",
        "\n",
        "\n",
        "# Load CSI data in a stratified sampling fashion\n",
        "def check_and_write(data_arr, labels_arr, data0, labels0, data1, labels1, fold, previous):\n",
        "  if not data0 is None:\n",
        "    print('Adding to subsets (zeros): Collector ' + previous, str(data0.shape))\n",
        "    p0 = np.random.permutation(data0.shape[0])\n",
        "    data0 = data0[p0]\n",
        "    labels0 = labels0[p0]\n",
        "    step_size0 = data0.shape[0]//fold\n",
        "  if not data1 is None:\n",
        "    print('Adding to subsets (ones): Collector ' + previous, str(data1.shape))\n",
        "    p1 = np.random.permutation(data1.shape[0])\n",
        "    data1 = data1[p1]\n",
        "    labels1 = labels1[p1]\n",
        "    step_size1 = data1.shape[0]//fold\n",
        "  # watch out if step size is zero (should not happen for our experiments)\n",
        "  # p is used to shuffle one more time when adding stratums to subsets\n",
        "  p = np.random.permutation(fold)\n",
        "  if not not data_arr:\n",
        "    for i in range(fold-1):\n",
        "      print(\"Loading \" + 'data_arr ' + str(p[i]))\n",
        "      if not data0 is None:\n",
        "        print('Zero part: Adding to ' + 'data_arr ' + str(p[i]) + '; Former shape: ' + str(data_arr[p[i]].shape), 'Adding: ' + str(data0[i*step_size0:(i+1)*step_size0].shape))\n",
        "        data_arr[p[i]] = np.concatenate((data_arr[p[i]],data0[i*step_size0:(i+1)*step_size0]), axis=0)\n",
        "        labels_arr[p[i]] = np.concatenate((labels_arr[p[i]],labels0[i*step_size0:(i+1)*step_size0]), axis=0)\n",
        "      if not data1 is None:\n",
        "        print('One part: Adding to ' + 'data_arr ' + str(p[i]) + '; Former shape: ' + str(data_arr[p[i]].shape), 'Adding: ' + str(data1[i*step_size1:(i+1)*step_size1].shape))\n",
        "        data_arr[p[i]] = np.concatenate((data_arr[p[i]],data1[i*step_size1:(i+1)*step_size1]), axis=0)\n",
        "        labels_arr[p[i]] = np.concatenate((labels_arr[p[i]],labels1[i*step_size1:(i+1)*step_size1]), axis=0)\n",
        "    if not data0 is None:\n",
        "      print('Zero part: Adding to ' + 'csi_' + str(p[fold-1]) + '; Former shape: ' + str(data_arr[p[fold-1]].shape), 'Adding: ' + str(data0[(fold-1)*step_size0:].shape))\n",
        "      data_arr[p[fold-1]] = np.concatenate((data_arr[p[fold-1]],data0[(fold-1)*step_size0:]), axis=0)\n",
        "      labels_arr[p[fold-1]] = np.concatenate((labels_arr[p[fold-1]],labels0[(fold-1)*step_size0:]), axis=0)\n",
        "    if not data1 is None:\n",
        "      print('One part: Following added to ' + 'csi_' + str(p[fold-1]) + '; Former shape: ' + str(data_arr[p[fold-1]].shape), 'Adding: ' + str(data1[(fold-1)*step_size1:].shape))\n",
        "      data_arr[p[fold-1]] = np.concatenate((data_arr[p[fold-1]],data1[(fold-1)*step_size1:]), axis=0)\n",
        "      labels_arr[p[fold-1]] = np.concatenate((labels_arr[p[fold-1]],labels1[(fold-1)*step_size1:]), axis=0)\n",
        "  else:\n",
        "    if not data0 is None and not data1 is None:\n",
        "      for i in range(fold-1):\n",
        "        data_arr.append(data0[i*step_size0:(i+1)*step_size0])\n",
        "        labels_arr.append(labels0[i*step_size0:(i+1)*step_size0])\n",
        "        data_arr[i] = np.concatenate((data_arr[i],data1[i*step_size1:(i+1)*step_size1]), axis=0)\n",
        "        labels_arr[i] = np.concatenate((labels_arr[i],labels1[i*step_size1:(i+1)*step_size1]), axis=0)\n",
        "        print('Writing csi_' + str(p[i]), str(data_arr[i].shape))\n",
        "      data_arr.append(data0[(fold-1)*step_size0:])\n",
        "      labels_arr.append(labels0[(fold-1)*step_size0:])\n",
        "      data_arr[fold-1] = np.concatenate((data_arr[fold-1],data1[(fold-1)*step_size1:]), axis=0)\n",
        "      labels_arr[fold-1] = np.concatenate((labels_arr[fold-1],labels1[(fold-1)*step_size1:]), axis=0)\n",
        "      print('Writing csi_' + str(p[fold-1]), str(data_arr[fold-1].shape))\n",
        "    elif not data0 is None:\n",
        "      for i in range(fold-1):\n",
        "        data_arr.append(data0[i*step_size0:(i+1)*step_size0])\n",
        "        labels_arr.append(labels0[i*step_size0:(i+1)*step_size0])\n",
        "      data_arr.append(data0[(fold-1)*step_size0:])\n",
        "      labels_arr.append(labels0[(fold-1)*step_size0:])\n",
        "    elif not data1 is None:\n",
        "      for i in range(fold-1):\n",
        "        data_arr.append(data1[i*step_size1:(i+1)*step_size1])\n",
        "        labels_arr.append(labels1[i*step_size1:(i+1)*step_size1])\n",
        "      data_arr.append(data1[(fold-1)*step_size1:])\n",
        "      labels_arr.append(labels1[(fold-1)*step_size1:])\n",
        "\n",
        "\n",
        "# Form training and test sets of CSI data, important:\n",
        "# (1) On Google Drive the CSI data is supposed to be stored at: My Drive/csi/data/ (e.g., My Drive/csi/data/6P)\n",
        "# (2) Here we use only CSI phases\n",
        "def validation_sets_from_data(directories, fold, seed):\n",
        "  np.random.seed(seed)\n",
        "  for directory in directories:\n",
        "    path = '/content/drive/My Drive/csi-zia/data/' + directory\n",
        "    relpath = '/content/drive/My Drive/csi-zia/data/' + directory + '/'\n",
        "    previous = 'None'\n",
        "    data0 = None\n",
        "    data1 = None\n",
        "    labels0 = None\n",
        "    labels1 = None\n",
        "    for filename in sorted(os.listdir(path)):\n",
        "      if filename.endswith('.txt') and 'csi' in filename:\n",
        "        #check if file belongs to new collector:\n",
        "        collector = filename.split('_')[0]\n",
        "        if collector == '':\n",
        "          collector = filename.split('_')[1]\n",
        "        if not previous==collector:\n",
        "          if not previous=='None':\n",
        "            check_and_write(csi_phase, phase_labels, data0, labels0, data1, labels1, fold, previous)\n",
        "          data0 = None\n",
        "          data1 = None\n",
        "          previous=collector\n",
        "        #handle new file:  \n",
        "        new = delete_null(np.loadtxt(relpath + filename))\n",
        "        new_labels = np.loadtxt(relpath + filename.replace('csi','labels')) \n",
        "        new_flatten = new[:,new.shape[1]//2:]                                     # use only CSI phases\n",
        "        if new_labels[0]==1: \n",
        "          if data1 is None:\n",
        "            data1 = new_flatten\n",
        "            labels1 = new_labels\n",
        "          else:  \n",
        "            data1 = np.concatenate((data1,new_flatten), axis=0)\n",
        "            labels1 = np.concatenate((labels1,new_labels), axis=0)\n",
        "        else:\n",
        "          if data0 is None:\n",
        "            data0 = new_flatten\n",
        "            labels0 = new_labels\n",
        "          else:  \n",
        "            data0 = np.concatenate((data0,new_flatten), axis=0)\n",
        "            labels0 = np.concatenate((labels0,new_labels), axis=0)\n",
        "    check_and_write(csi_phase, phase_labels, data0, labels0, data1, labels1, fold, previous)\n",
        "  print('Data successfully rearranged.')\n",
        "\n",
        "\n",
        "# *****************************************************************************#\n",
        "\n",
        "# Forming the data by calling the above functions: give explicit names for phase\n",
        "csi_phase = []      # CSI phase data\n",
        "phase_labels = []   # Phase labels: 1 - for colocated devices, 0 - for non-colocated devices\n",
        "\n",
        "# Iterate over the experiments (see above Office data is composed from multiple data sets), important:\n",
        "# (1) We set the number of folds to 5, i.e., for 5-fold CV in 'validation_sets_from_data' and \n",
        "# (2) The seed to shuffle the data to 123, for reproducibility (same function)\n",
        "for experiment in experiments:\n",
        "  print(experiment)\n",
        "  validation_sets_from_data(experiment, 5, 123)\n",
        "  print(experiment[0].split('/')[0] + '/' + experiment[0].split('/')[1] + ' subsets finished') \n",
        "\n",
        "print(csi_phase[0].shape, csi_phase[1].shape, csi_phase[2].shape, csi_phase[3].shape, csi_phase[4].shape)\n",
        "print(phase_labels[0].shape, phase_labels[1].shape, phase_labels[2].shape, phase_labels[3].shape, phase_labels[4].shape)\n",
        "print('')\n",
        "print('')\n",
        "\n",
        "# Store sanitized phases (for each fold similar to csi_phase above)\n",
        "sanitized_phase = []"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SfyPQ1Wd_C68"
      },
      "source": [
        "To perform *phase santitization* **execute** the below cell, otherwise if you want to use *raw phases*—**skip** this cell and proceed to the next one. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KzBbBPc4_Dfy"
      },
      "source": [
        "# Carrier indices for 2.4 GHz\n",
        "carrier_idx = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, \n",
        "               23, 24, 25, 26, 27, 28, -28, -27, -26, -25, -24, -23, -22, -21, -20, -19, -18, \n",
        "               -17, -16, -15, -14, -13, -12, -11, -10, -9, -8, -7, -6, -5, -4, -3, -2, -1]\n",
        "\n",
        "\n",
        "def phase_sanitization(raw_phase):\n",
        "    # Iterate over folds\n",
        "    for fold in raw_phase:\n",
        "        # Fold data (i.e., phases)\n",
        "        fold_phase = []\n",
        "\n",
        "        # Iterate over each CSI measurement in the fold\n",
        "        for meas in fold:\n",
        "            # Construct dict with carrier idx (keys) and phases (values)\n",
        "            p = dict(zip(carrier_idx, meas))\n",
        "\n",
        "            # Store values for one CSI measurement\n",
        "            meas_phase = []\n",
        "            \n",
        "            # Iterate over p\n",
        "            for k, v in p.items():\n",
        "                a = (p[28] - p[-28]) / len(p) * k\n",
        "                b = 1 / len(p) * np.sum(meas)\n",
        "                meas_phase.append(v - a - b)\n",
        "\n",
        "            # Add santized phases of one CSI measurement\n",
        "            fold_phase.append(meas_phase)\n",
        "\n",
        "        # Add sanitized phases of one fold\n",
        "        sanitized_phase.append(np.array(fold_phase))\n",
        "\n",
        " \n",
        "# Call 'phase_sanitization'\n",
        "phase_sanitization(csi_phase)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zsyf2bpKIJBl"
      },
      "source": [
        "Form the training and test sets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dDA6oe3MInAB"
      },
      "source": [
        "# Check if 'sanitized_phase' contains data, then we should work with it\n",
        "if sanitized_phase:\n",
        "  csi_phase = sanitized_phase\n",
        "\n",
        "# Forming the training and test datasets\n",
        "for i in range(1):\n",
        "  test_data = csi_phase[i]\n",
        "  test_labels = phase_labels[i]\n",
        "  \n",
        "  train_data = csi_phase[(i+1)%5]\n",
        "  train_labels = phase_labels[(i+1)%5]\n",
        "  for k in range(5):\n",
        "    if not k==i and not k==(i+1)%5:\n",
        "      train_data = np.concatenate((train_data, csi_phase[k]), axis=0)\n",
        "      train_labels = np.concatenate((train_labels, phase_labels[k]), axis=0)\n",
        "  print(train_data.shape, train_labels.shape)\n",
        "  print(test_data.shape, test_labels.shape)\n",
        "\n",
        "  # Normalizing CSI data using variance scaling\n",
        "  mean = np.mean(train_data, axis=0)\n",
        "  train_data = train_data - mean\n",
        "  std = np.std(train_data, axis=0)\n",
        "  train_data = train_data / std\n",
        "  test_data = (test_data - mean) / std"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xS_4PmjwQy_B"
      },
      "source": [
        "Train on the CSI phases (i.e., 80% to 20% setup) and make copresence predictions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jxXOQE3li0VG"
      },
      "source": [
        "train_acc, test_acc, train_pred, test_pred, model = nn_model_train_predict(train_data, train_labels, test_data, test_labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IqL79NKJRAGh"
      },
      "source": [
        "Display the results: accuracy and AUC."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yqGXfKGiRSVc"
      },
      "source": [
        "print('Accuracy: %.4f' % test_acc)\n",
        "print('AUC: %.4f' % roc_auc_score(test_labels, test_pred))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CH70ecOhIXOx"
      },
      "source": [
        "## Right for the Right Reasons (RRR) code taken from the original [paper](https://github.com/dtak/rrr). "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eISjDfUNbCrG"
      },
      "source": [
        "import tensorflow.compat.v1 as tf\n",
        "tf.disable_v2_behavior()\n",
        "import numpy as np\n",
        "\n",
        "to_logprob = lambda L: L - tf.reduce_logsumexp(L, axis=1, keep_dims=True)\n",
        "\n",
        "def one_hot(y):\n",
        "  if len(y.shape) != 1:\n",
        "    return y\n",
        "  values = np.array(sorted(list(set(y))))\n",
        "  return np.array([values == v for v in y], dtype=np.uint8)\n",
        "\n",
        "class TensorflowPerceptron():\n",
        "  def loss_function(self, l2_grads=1000, l1_grads=0, l2_params=0.0001):\n",
        "    right_answer_loss = tf.reduce_sum(tf.multiply(self.y, -self.log_prob_ys))\n",
        "\n",
        "    gradXes = tf.gradients(self.log_prob_ys, self.X)[0]\n",
        "    A_gradX = tf.multiply(self.A, gradXes)\n",
        "    right_reason_loss = 0\n",
        "    if l1_grads > 0:\n",
        "      right_reason_loss += l1_grads * tf.reduce_sum(tf.abs(A_gradX))\n",
        "    if l2_grads > 0:\n",
        "      right_reason_loss += l2_grads * tf.nn.l2_loss(A_gradX)\n",
        "\n",
        "    small_params_loss = l2_params * tf.add_n([tf.nn.l2_loss(p) for p in self.W + self.b])\n",
        "\n",
        "    return right_answer_loss + right_reason_loss + small_params_loss\n",
        "\n",
        "  def optimizer(self, l2_grads=1000, l1_grads=0, l2_params=0.0001, learning_rate=0.001):\n",
        "    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
        "    return optimizer.minimize(self.loss_function(l2_grads=l2_grads, l1_grads=l1_grads, l2_params=l2_params))\n",
        "\n",
        "  def fit(self, X, y, A=None,\n",
        "      hidden_layers=[50,30], nonlinearity=tf.nn.relu, weight_sd=0.1,\n",
        "      l2_grads=1000, l1_grads=0, l2_params=0.001,\n",
        "      num_epochs=64, batch_size=256, learning_rate=0.001):\n",
        "\n",
        "    y_org = y\n",
        "    y = one_hot(y)\n",
        "    y_dimensions = y.shape[1]\n",
        "    x_dimensions = X.shape[1]\n",
        "    num_examples = X.shape[0]\n",
        "    if A is None:\n",
        "      A = np.zeros((num_examples, x_dimensions))\n",
        "    assert(num_examples == y.shape[0])\n",
        "    assert(A.shape == X.shape)\n",
        "\n",
        "    # set up network\n",
        "    self.layer_sizes = [x_dimensions] + list(hidden_layers) + [y_dimensions]\n",
        "    self.X = tf.placeholder(\"float\", [None, x_dimensions], name=\"X\")\n",
        "    self.A = tf.placeholder(\"float\", [None, x_dimensions], name=\"A\")\n",
        "    self.y = tf.placeholder(\"float\", [None, y_dimensions], name=\"y\")\n",
        "    self.W = []\n",
        "    self.b = []\n",
        "    self.L = [self.X]\n",
        "    for i in range(1, len(self.layer_sizes)):\n",
        "      self.W.append(tf.Variable(tf.random_normal(self.layer_sizes[i-1:i+1], stddev=weight_sd), name='W{}'.format(i)))\n",
        "      self.b.append(tf.Variable(tf.random_normal([self.layer_sizes[i]], stddev=weight_sd), name='b{}'.format(i)))\n",
        "    for i, activation in enumerate([nonlinearity for _ in hidden_layers] + [to_logprob]):\n",
        "      self.L.append(activation(tf.add(tf.matmul(self.L[i], self.W[i]), self.b[i])))\n",
        "\n",
        "    # Set up optimization\n",
        "    optimizer = self.optimizer(learning_rate=learning_rate, l2_grads=l2_grads, l1_grads=l1_grads, l2_params=l2_params)\n",
        "    batch_size = min(batch_size, num_examples)\n",
        "    num_batches = int(np.ceil(num_examples / batch_size))\n",
        "\n",
        "    with tf.Session() as sess:\n",
        "      sess.run(tf.global_variables_initializer())\n",
        "      for i in range(num_epochs):\n",
        "        for k in range(num_batches):\n",
        "          idx = slice(k*batch_size, (k+1)*batch_size)\n",
        "          sess.run(optimizer, feed_dict={self.X: X[idx], self.y: y[idx], self.A: A[idx]})\n",
        "        print(\"Epoch {0} -> Accuracy: {1:.2f}%\".format(i,self.score(X, y_org, from_session=True)*100))\n",
        "      self.W_vals = [weights.eval() for weights in self.W]\n",
        "      self.b_vals = [biases.eval() for biases in self.b]\n",
        "  \n",
        "  def _initialize_variables(self, sess):\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "    for var, val in zip(self.W + self.b, self.W_vals + self.b_vals):\n",
        "      sess.run(var.assign(val))\n",
        "\n",
        "  @property\n",
        "  def log_prob_ys(self):\n",
        "    return self.L[-1]\n",
        "\n",
        "  def input_gradients(self, X, y=None, log_scale=True):\n",
        "    with tf.Session() as session:\n",
        "      self._initialize_variables(session)\n",
        "      probs = self.log_prob_ys\n",
        "      if y is not None: probs = probs[:,y]\n",
        "      if not log_scale: probs = tf.exp(probs)\n",
        "      grads = tf.gradients(probs, self.X)[0].eval(feed_dict={self.X: X})\n",
        "    return grads\n",
        "\n",
        "  def largest_gradient_mask(self, X, cutoff=0.67, **kwargs):\n",
        "    grads = self.input_gradients(X, **kwargs)\n",
        "    return np.array([np.abs(g) > cutoff*np.abs(g).max() for g in grads])\n",
        "\n",
        "  def predict_log_proba(self, X, from_session=False):  \n",
        "    if not from_session:\n",
        "      with tf.Session() as session:\n",
        "        self._initialize_variables(session)\n",
        "        log_probs = self.log_prob_ys.eval(feed_dict={self.X: X})\n",
        "    else:\n",
        "      log_probs = self.log_prob_ys.eval(feed_dict={self.X: X})\n",
        "    return log_probs\n",
        "\n",
        "  def predict(self, X, **kwargs):\n",
        "    return np.argmax(self.predict_log_proba(X, **kwargs), axis=1)\n",
        "\n",
        "  def predict_proba(self, X):\n",
        "    return np.exp(self.predict_log_proba(X), axis=1)\n",
        "\n",
        "  def score(self, X, y, **kwargs):\n",
        "    return np.mean(self.predict(X, **kwargs) == y)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N311WT7QOUsl"
      },
      "source": [
        "## Logic for applying the RRR method to find which features are considered important by a neural network in the copresence decision task (cf. Section 6.4 in the paper)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z_3u7msePCD7"
      },
      "source": [
        "# *************************** Core logic *******************************#\n",
        "def get_relevant_features(model, train_set, test_set=None, cutoff=0.67):\n",
        "  # Print current size of train and test sets\n",
        "  if test_set is not None:\n",
        "    print(train_set.shape)\n",
        "    print(test_set.shape)\n",
        "    print()\n",
        "\n",
        "  # Obtain gradient mask for the train set\n",
        "  mask = model.largest_gradient_mask(train_set, cutoff=cutoff)\n",
        "\n",
        "  # Get mask for positive predictions\n",
        "  pos_mask = mask\n",
        "\n",
        "  # Count feature occurrence in the positive mask\n",
        "  mask_count = np.sum(pos_mask, axis=0)\n",
        "\n",
        "  # Store feature importance\n",
        "  feature_import = {}\n",
        "\n",
        "  # Iterate over features\n",
        "  indices = np.argsort(mask_count)\n",
        "  for k in indices[::-1]:\n",
        "    # print(\"{:<8} {:<10} {:<10}\".format(k, mask_count[k], mask_count[k] / pos_mask.shape[0] * 100))\n",
        "    feature_import[k] = mask_count[k] / pos_mask.shape[0] * 100\n",
        "\n",
        "  return feature_import\n",
        "\n",
        "\n",
        "def relevant_features_list(feature_import, rel_thr):\n",
        "  # Feature list\n",
        "  feature_list = []\n",
        "\n",
        "  # Iterate over feature_import\n",
        "  for k,v in feature_import.items():\n",
        "    if v > rel_thr:\n",
        "      feature_list.append(k)\n",
        "\n",
        "  print(len(feature_list), feature_list)\n",
        "  return feature_list\n",
        "\n",
        "\n",
        "def del_relevant_features(train_set, test_set, feature_list):\n",
        "  # Reduced train and test sets\n",
        "  red_train_set = np.delete(train_set, np.array(feature_list), 1)\n",
        "  red_test_set = np.delete(test_set, np.array(feature_list), 1)\n",
        "\n",
        "  # Print sizes of reduced sets\n",
        "  print(red_train_set.shape)\n",
        "  print(red_test_set.shape)\n",
        "\n",
        "  return red_train_set, red_test_set\n",
        "\n",
        "\n",
        "def train_rrr_model(train_set, train_labels, test_set, test_labels, mask=None):\n",
        "  tf.compat.v1.random.set_random_seed(123)\n",
        "\n",
        "  model = TensorflowPerceptron()\n",
        "  model.fit(train_set, train_labels, hidden_layers=[300,100,50], num_epochs=50, \n",
        "            A=mask)\n",
        "  print(model.score(test_set, test_labels))\n",
        "\n",
        "  return model\n",
        "\n",
        "\n",
        "def construct_A_matrix(train_set, feature_list, A_mask=None):\n",
        "  if A_mask is None:\n",
        "    # Define A matrix\n",
        "    A_mask = np.zeros(train_set.shape, dtype=np.uint8)\n",
        "  \n",
        "  # Iterate over feature list\n",
        "  for f in feature_list:\n",
        "    # Set the coloumn corresponding to f to 1s\n",
        "    A_mask[:,f] = 1\n",
        "  \n",
        "  return A_mask\n",
        "\n",
        "\n",
        "def flip_bin_mask(bin_mask):\n",
        "  # Make a copy of bin_mask\n",
        "  flipped_bin_mask = np.copy(bin_mask)\n",
        "\n",
        "  # Get boolean arrays for 0s and 1s positions\n",
        "  zero_pos = (bin_mask == 0)\n",
        "  one_pos = (bin_mask == 1)\n",
        "\n",
        "  # Flip 0s and 1s\n",
        "  flipped_bin_mask[zero_pos] = 1\n",
        "  flipped_bin_mask[one_pos] = 0\n",
        "\n",
        "  return flipped_bin_mask\n",
        "\n",
        "\n",
        "def apply_bin_mask(data, bin_mask):\n",
        "  return data * bin_mask[:data.shape[0],]\n",
        "\n",
        "\n",
        "# *************************** Helpers *******************************#\n",
        "def idx_to_str(idx):\n",
        "    # Make it 01, 02, etc.\n",
        "    if idx < 10:\n",
        "        return '0' + str(idx)\n",
        "\n",
        "    return str(idx)\n",
        "\n",
        "def myconverter(obj):\n",
        "        if isinstance(obj, np.integer):\n",
        "            return int(obj)\n",
        "        elif isinstance(obj, np.floating):\n",
        "            return float(obj)\n",
        "        elif isinstance(obj, np.ndarray):\n",
        "            return obj.tolist()\n",
        "        elif isinstance(obj, datetime.datetime):\n",
        "            return obj.__str__()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6mpI-2v9STfR"
      },
      "source": [
        "Find important features with the RRR method.\n",
        "\n",
        "**Prerequisite:** requires execution of the following cells in the following order:\n",
        "\n",
        "\n",
        "1.   'Normal training' --> Load CSI data of the desired experiment. \n",
        "2.   'Train a neural network model with Keras' --> Keras functionality. \n",
        "3.   'Right for the Right Reasons (RRR) code taken from the original paper'.\n",
        "4.   'Logic for applying the RRR method'.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x8J90CIGSUOi"
      },
      "source": [
        "from json import dumps\n",
        "import pathlib\n",
        "\n",
        "# Initilize vars\n",
        "A_mask=None\n",
        "Af_mask=None\n",
        "tf.compat.v1.random.set_random_seed(123)\n",
        "kr_epochs = 35\n",
        "tf_epochs = 50\n",
        "idx = 0\n",
        "auc_thr = 0.85\n",
        "res = {}\n",
        "feature_list = []\n",
        "log_path = '/content/drive/My Drive/csi-zia/results/rrr-logs'\n",
        "model_path = '/content/drive/My Drive/csi-zia/results/rrr-models'\n",
        "\n",
        "# Iterate until convergence\n",
        "while(1):\n",
        "\n",
        "  # Train Keras model\n",
        "  if A_mask is None:\n",
        "    _, _, _, test_pred, kr_model = nn_model_train_predict(train_data, train_labels, \n",
        "                                                          test_data, test_labels, \n",
        "                                                          kr_epochs)\n",
        "  else:\n",
        "    _, _, _, test_pred, kr_model = nn_model_train_predict(apply_bin_mask(train_data, Af_mask), train_labels, \n",
        "                                      apply_bin_mask(test_data, Af_mask), test_labels, \n",
        "                                      kr_epochs)\n",
        "  \n",
        "  # Get Keras AUC\n",
        "  kr_auc = roc_auc_score(test_labels, test_pred)\n",
        "\n",
        "  # Train TensorFlow RRR model\n",
        "  model = TensorflowPerceptron()\n",
        "  model.fit(train_data, train_labels, A=A_mask, hidden_layers=[300,100,50], \n",
        "            num_epochs=tf_epochs)\n",
        "\n",
        "  # Get TensorFlow AUC\n",
        "  tf_auc = roc_auc_score(test_labels, model.predict(test_data))\n",
        "\n",
        "  # Store results\n",
        "  res[idx_to_str(idx)] = {'Keras': float(kr_auc), 'TF': float(tf_auc), \n",
        "                          'Feat': feature_list, 'Feat_n': len(feature_list)}\n",
        "  \n",
        "  # Construct model filename\n",
        "  model_filename = experiments[0][0].split('/')\n",
        "  model_filename = model_filename[0].lower() + '-' + \\\n",
        "  model_filename[1].split(' ')[0] + '_' + idx_to_str(idx)\n",
        "\n",
        "  # Workaround for the office filepath\n",
        "  if 'office' in model_filename:\n",
        "    folder = experiments[0][0].split('/')\n",
        "    folder = folder[0] + '/' + folder[1]\n",
        "  else:\n",
        "    folder = experiments[0][0]\n",
        "  \n",
        "  # Save Keras model\n",
        "  kr_model.save(model_path + '/' + folder + '/' + model_filename + '.h5')\n",
        "\n",
        "  # Get feature importance\n",
        "  feature_import = get_relevant_features(model, train_data, test_data)\n",
        "\n",
        "  # Get features that have realtive improtance over 10%\n",
        "  feature_list = relevant_features_list(feature_import, 10)\n",
        "\n",
        "  # Construct A_mask for Tensorflow's RRR\n",
        "  if A_mask is None:\n",
        "    A_mask = construct_A_matrix(train_data, feature_list)\n",
        "  else:\n",
        "    A_mask = construct_A_matrix(train_data, feature_list, A_mask)\n",
        "\n",
        "  # Construct Af_mask for Keras\n",
        "  Af_mask = flip_bin_mask(A_mask)\n",
        "\n",
        "  # Leave while loop\n",
        "  if kr_auc < auc_thr:\n",
        "    break\n",
        "  \n",
        "  # Increment idx\n",
        "  idx += 1\n",
        "\n",
        "# Construct log filename\n",
        "log_filename = experiments[0][0].split('/')\n",
        "log_filename = log_filename[0].lower() + '-' + log_filename[1].split(' ')[0]\n",
        "\n",
        "# Workaround for the office filepath\n",
        "if 'office' in log_filename:\n",
        "  folder = experiments[0][0].split('/')\n",
        "  folder = folder[0] + '/' + folder[1]\n",
        "else:\n",
        "  folder = experiments[0][0]\n",
        "\n",
        "# Create full path to save log file if it does not exist\n",
        "pathlib.Path(log_path + '/' + folder + '/').mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Save results to a JSON file on Google drive\n",
        "with open(log_path + '/' + folder + '/' + log_filename + '.json', 'w') as f:\n",
        "    f.write(dumps(res, default=myconverter, indent=4, sort_keys=True)) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WIgYINBJSaqs"
      },
      "source": [
        "## Generalizability (cf. Section 6.3 in the paper)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PzwRsnJqbdPi"
      },
      "source": [
        "###Train or load the full office model. \n",
        "\n",
        "To train the full office model, execute the following cells in the following order before running the next cell:\n",
        "\n",
        "1.  'Choose the CSI data' --> uncomment 'experiments' for the office. \n",
        "2.  'Normal training'.\n",
        "3.  'Train a neural network model with Keras'.\n",
        "\n",
        "To load the prevously trained full office model (only an example), execute the cell after the next one."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DYz59zYFSa5B"
      },
      "source": [
        "# Train office model\n",
        "train_acc, test_acc, train_pred, test_pred, model = nn_model_train_predict(train_data, train_labels, test_data, test_labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rYnb-2BHZUZp"
      },
      "source": [
        "# Check the office's model accuracy and AUC\n",
        "print('Accuracy: %.4f' % test_acc)\n",
        "print('AUC: %.4f' % roc_auc_score(test_labels, test_pred))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZWcIjHAVeZRg"
      },
      "source": [
        "from keras.models import load_model\n",
        "from glob import glob\n",
        "\n",
        "# Load office model\n",
        "\n",
        "# Model path (just an example, there is no model)\n",
        "model_path = '/content/drive/My Drive/csi-zia/full-models/Office/2400 MHz'\n",
        "\n",
        "# Read model file\n",
        "model_file = glob(model_path  + '/*.h5', recursive=True)\n",
        "\n",
        "# Load model\n",
        "model = tf.keras.models.load_model(model_file[0], \n",
        "                                   custom_objects={'leaky_relu': tf.nn.leaky_relu})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x4zJuFVKfl8E"
      },
      "source": [
        "### Now load the data of another experiment (other than the office) to evaluate the generalizability, i.e., execute the following cells before running the next one: \n",
        "\n",
        "1.  'Choose the CSI data' --> uncomment 'experiments' for the desired experiment. \n",
        "2.  'Normal training'."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2YrPrzqIi1gD"
      },
      "source": [
        "Set the first two layers in the trained office model to be nontrainable and recompile the model. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d5GTUUCjiwIO"
      },
      "source": [
        "# Set initial layers to trainable=False\n",
        "for l in model.layers:\n",
        "  if l.output_shape[1] > 100 and isinstance(l, keras.layers.Dense):\n",
        "    l.trainable = False\n",
        "  print(l.name, l.output_shape, l.trainable)\n",
        "\n",
        "# Compile the updated model\n",
        "model.compile(optimizer='adam',\n",
        "            loss='sparse_categorical_crossentropy',\n",
        "            metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gs3szYobjdzP"
      },
      "source": [
        "Retrain the last layers of our neural network model on the loaded data from another (i.e., other than the office) experiment. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "heyub4b0jeiK"
      },
      "source": [
        "# Perform training\n",
        "model.fit(train_data, train_labels, verbose=2, epochs=10, batch_size=32)\n",
        "\n",
        "# Get training and test accuracies\n",
        "train_loss, train_acc = model.evaluate(train_data, train_labels)\n",
        "test_loss, test_acc = model.evaluate(test_data, test_labels)\n",
        "\n",
        "# Make training and test soft predictions\n",
        "train_pred_soft = model.predict(train_data)\n",
        "test_pred_soft = model.predict(test_data)\n",
        "\n",
        "# Convert soft predictions to binary predictions\n",
        "train_pred = make_bin_predictions(train_pred_soft)\n",
        "test_pred = make_bin_predictions(test_pred_soft)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cNQFcxedkMCc"
      },
      "source": [
        "Display the results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wPNlRO6mkMPq"
      },
      "source": [
        "print('%s: AUC = %.4f' % (experiments[0][0].split('/')[0], \n",
        "                             roc_auc_score(test_labels, test_pred)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QiMyEeuCdEOQ"
      },
      "source": [
        "# Advanced attack scenarios (cf. Section 6.5 in the paper)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Gjgwh7cOTy0"
      },
      "source": [
        "### Data precollection attack. \n",
        "\n",
        "**Description:** use the moving cars model to classify the copresence on the parked car data. To defend against this attack, we employ a number of Right for the Right Reasons (RRR) neural network models. \n",
        "\n",
        "**Prerequisite:** Before running the below cell, execute the following cells in the following order:\n",
        "\n",
        "1.  'Choose the CSI data' --> uncomment 'experiments' for the parking cars. \n",
        "2.  'Normal training'.\n",
        "3.  'Train a neural network model with Keras'.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wxtpJFLOCIMy"
      },
      "source": [
        "Load RRR models for the moving cars scenario."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qsg7KmuzALr9"
      },
      "source": [
        "# Load necessary modules\n",
        "from keras.models import load_model\n",
        "from glob import glob\n",
        "\n",
        "# Load scenario RRR models, can be found here: https://doi.org/10.5281/zenodo.5592823\n",
        "model_path = '/content/drive/My Drive/csi-zia/results/rrr-models/Moving Cars/'\n",
        "\n",
        "# Read model files\n",
        "model_files = glob(model_path + experiment[0].split('/')[1] + '/*.h5', \n",
        "                   recursive=True)\n",
        "\n",
        "# List to store models\n",
        "models = []\n",
        "\n",
        "# Iterate over models\n",
        "for mf in model_files:\n",
        "  # Load a model\n",
        "  m = tf.keras.models.load_model(mf, \n",
        "                                 custom_objects={'leaky_relu': tf.nn.leaky_relu})\n",
        "  \n",
        "  # Append to list\n",
        "  models.append(m)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xp07CorSCLdx"
      },
      "source": [
        "Apply the loaded RRR models on the parked cars data and display the results."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N11iW9QnCWjY"
      },
      "source": [
        "# Helper function\n",
        "def idx_to_str(idx):\n",
        "    # Make it 01, 02, etc.\n",
        "    if idx < 10:\n",
        "        return '0' + str(idx)\n",
        "\n",
        "    return str(idx)\n",
        "\n",
        "# Idx to track models\n",
        "idx = 0\n",
        "\n",
        "# Iterate over models\n",
        "for m in models:\n",
        "  test_pred_soft = m.predict(test_data)\n",
        "\n",
        "  # Convert soft predictions to binary predictions\n",
        "  test_pred = make_bin_predictions(test_pred_soft)\n",
        "\n",
        "  print('%s: AUC = %.4f' % (idx_to_str(idx), \n",
        "                            roc_auc_score(test_labels, test_pred)))\n",
        "  idx += 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mtYJI4FQDqnZ"
      },
      "source": [
        "### Increased power attack.\n",
        "\n",
        "**Description:** use increased power CSI samples from the power scenario to evaluate the performance of our neural network on the unseen data. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f9oqys2jIm9D"
      },
      "source": [
        "Load office and power data --> this is just to speed things up. Alternatively, one can run the combination of 'Choose the CSI data'  and 'Normal training' two times for the office and power experiments, resulting in the same loaded data.\n",
        "\n",
        "**Prerequisite:** Before running the below cell, execute the 'Train a neural network model with Keras' cell. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HXVBByioIvIT"
      },
      "source": [
        "# Load office data, can be found here: https://doi.org/10.5281/zenodo.5592823\n",
        "data_path = '/content/drive/My Drive/csi-zia/power-attack/Office/2400 MHz'\n",
        "\n",
        "# Train and test data (office)\n",
        "o_train_data = np.load(data_path + '/o_train_data.npy')\n",
        "o_test_data = np.load(data_path + '/o_test_data.npy')\n",
        "\n",
        "# Train and test data (office)\n",
        "o_train_labels = np.load(data_path + '/o_train_labels.npy')\n",
        "o_test_labels = np.load(data_path + '/o_test_labels.npy')\n",
        "\n",
        "# Load power data, can be found here: https://doi.org/10.5281/zenodo.5592823\n",
        "data_path = '/content/drive/My Drive/csi-zia/power-attack/Power/2400 MHz'\n",
        "\n",
        "# Train and test data (power)\n",
        "p_train_data = np.load(data_path + '/p_train_data.npy')\n",
        "p_test_data = np.load(data_path + '/p_test_data.npy')\n",
        "\n",
        "# Train and test data (power)\n",
        "p_train_labels = np.load(data_path + '/p_train_labels.npy')\n",
        "p_test_labels = np.load(data_path + '/p_test_labels.npy')"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OHns0c9MTSVO"
      },
      "source": [
        "#### Now there are three options (execute one of the below cells at a time; to play safe, re-execute the above loading cell for each iteration):"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CLR87nFNQsJ1"
      },
      "source": [
        "(1) Combine training data from both office and power scenarios and make predictions on the power data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bb0KkpkHQsvN"
      },
      "source": [
        "# Combine office and power data\n",
        "train_data = np.concatenate((o_train_data, p_train_data), axis=0)\n",
        "train_labels = np.concatenate((o_train_labels, p_train_labels), axis=0)\n",
        "\n",
        "# Do the training / make predictions\n",
        "_, _, _, test_pred, _ = nn_model_train_predict(train_data, train_labels, \n",
        "                                               p_test_data, p_test_labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iEXclX_EgokJ"
      },
      "source": [
        "Display the results for (1)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bf6HF5-LgpDY"
      },
      "source": [
        "# Display the resulting AUC\n",
        "print('AUC: %.4f' % roc_auc_score(p_test_labels, test_pred))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "160axDrpg1VA"
      },
      "source": [
        "(2) Combine training data from both office and power scenarios, **excluding** the high-power samples from the power data (i.e., CSI data from non-colocated devices), and make predictions on the power data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "90Dehridg_Ep"
      },
      "source": [
        "# Function to exclude high power CSI data from the power scenario data\n",
        "def exclude_high_power_data(power_data, power_labels):\n",
        "  # Indices corresponding to CSI data of colocated devices, i.e., label == 1\n",
        "  col_csi_idx = []\n",
        "\n",
        "  # Find indices of CSI measurements corresponding to label 1\n",
        "  for i in range(len(power_labels)):\n",
        "    if int(power_labels[i]) == 1:\n",
        "      col_csi_idx.append(i)\n",
        "  \n",
        "  # Get only the data corresponding to colocated devices, i.e., \n",
        "  # exclude high-power CSI measurements of non-colocated devices \n",
        "  col_csi_data = power_data[col_csi_idx, :]\n",
        "\n",
        "  return col_csi_data\n",
        "\n",
        "\n",
        "# Exclude high-power data and prepare the combined training data\n",
        "exl_p_data = exclude_high_power_data(p_train_data, p_train_labels)\n",
        "\n",
        "train_data = np.concatenate((o_train_data, exl_p_data), axis=0)\n",
        "train_labels = np.concatenate((o_train_labels, np.ones(len(exl_p_data))), axis=0)\n",
        "\n",
        "# Do the training / make predictions\n",
        "_, _, _, test_pred, _ = nn_model_train_predict(train_data, train_labels, \n",
        "                                               p_test_data, p_test_labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5fFQoHM71ozK"
      },
      "source": [
        "Display the results for (2)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c_pfGHlZ1on0"
      },
      "source": [
        "# Display the resulting AUC\n",
        "print('AUC: %.4f' % roc_auc_score(p_test_labels, test_pred))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9GV_GSu5AiFv"
      },
      "source": [
        "(3) Combine training data from both office and power scenarios, **using only 10%** of high-power samples from the power data (i.e., CSI data from non-colocated devices), and make predictions on the power data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V4XfpO_eym_k"
      },
      "source": [
        "import random\n",
        "\n",
        "# Function to sample high power CSI data from the power scenario data\n",
        "def sample_high_power_data(power_data, power_labels, sample_size=0.1):\n",
        "  # Indices corresponding to CSI data of colocated devices, i.e., label == 1\n",
        "  col_csi_idx = []\n",
        "  \n",
        "  # Find indices of CSI measurements corresponding to label 1\n",
        "  for i in range(len(power_labels)):\n",
        "    if int(power_labels[i]) == 1:\n",
        "      col_csi_idx.append(i)\n",
        "  \n",
        "  # Get only the data corresponding to colocated devices, i.e., \n",
        "  # exclude high-power CSI measurements of non-colocated devices \n",
        "  col_csi_data = power_data[col_csi_idx, :]\n",
        "\n",
        "  # Get indicies of CSI measurements corresponding to label 0, i.e., non-colocated devices,\n",
        "  # transmitting with high power\n",
        "  ncol_csi_idx = list(set([x for x in range(len(power_labels))]) - set(col_csi_idx))\n",
        "\n",
        "  # Get a random sample size (default 10%) of high-power CSI data, i.e., indices\n",
        "  ncol_csi_idx_sample = random.sample(ncol_csi_idx, int(len(ncol_csi_idx) * sample_size))\n",
        "\n",
        "  # Get a sample of high-power CSI data\n",
        "  ncol_csi_data_sample = power_data[ncol_csi_idx_sample, :]\n",
        "\n",
        "  return col_csi_data, ncol_csi_data_sample\n",
        "\n",
        "\n",
        "# Use only a sample of high-power data and prepare the combined training data\n",
        "exl_p_data, samp_p_data = sample_high_power_data(p_train_data, p_train_labels)\n",
        "\n",
        "train_data = np.concatenate((o_train_data, exl_p_data, samp_p_data), axis=0)\n",
        "train_labels = np.concatenate((o_train_labels, np.ones(len(exl_p_data)), np.zeros(len(samp_p_data))), axis=0)\n",
        "\n",
        "# Do the training / make predictions\n",
        "_, _, _, test_pred, _ = nn_model_train_predict(train_data, train_labels, \n",
        "                                               p_test_data, p_test_labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2z6NkeL2A0N9"
      },
      "source": [
        "Display the results for (3)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FT7YzcAPA0xu"
      },
      "source": [
        "# Display the resulting AUC\n",
        "print('AUC: %.4f' % roc_auc_score(p_test_labels, test_pred))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}